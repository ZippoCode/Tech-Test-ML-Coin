{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50SbNLEo_Ylw"
      },
      "source": [
        "# Download and extract dataset\n",
        "\n",
        "As an initial step, it is of essential importance to download the [Public Coin Dataset](https://drive.google.com/file/d/1cF-u9N7miok5-KUiriTdvA62edxIs1I_) and extract its contents. Moreover, we retrieve the parameters from the previously executed training sessions, which will be instrumental in our ongoing work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TRlJ8dezdkV",
        "outputId": "4f927b44-b24c-4b15-d009-2328be700a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cF-u9N7miok5-KUiriTdvA62edxIs1I_\n",
            "To: /content/public_coin_dataset.zip\n",
            "100% 40.6M/40.6M [00:00<00:00, 41.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RaRbCZHKc8kIeI6KaznPxO_9VM3tE6TP\n",
            "To: /content/model_checkpoints_pretrained.pth\n",
            "100% 62.2M/62.2M [00:02<00:00, 30.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1cF-u9N7miok5-KUiriTdvA62edxIs1I_ -O public_coin_dataset.zip\n",
        "!gdown 1RaRbCZHKc8kIeI6KaznPxO_9VM3tE6TP -O model_checkpoints_pretrained.pth\n",
        "!unzip -qq -o public_coin_dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skc6nlbAcFfg"
      },
      "source": [
        "# Importing\n",
        "Importing all the libraries utilized in the tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mUhoNRppcI-q"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn.functional import pad\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms.functional import hflip, vflip, resize\n",
        "from torchvision.transforms.transforms import Compose, RandomHorizontalFlip, RandomVerticalFlip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "I decided to employ the [U-net](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/) neural network architecture for the specific segmentation task, known for its effectiveness in image segmentation and feature extraction. \\\\\n",
        "\n",
        "This U-Net model consists of a multi-level structure with four levels of blocks. Each block includes a pair of convolutional layers with batch normalization and ReLU activation functions. There is one additional max-pooling layer in the encoding part, while in the decoding part, up-convolutional layers are utilized instead."
      ],
      "metadata": {
        "id": "-ekHgMmcd6s2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu2lTyKt4s5a",
        "outputId": "1d93f8bc-7f1f-46dd-990c-d4bef8e0ef0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/mateuszbuda/brain-segmentation-pytorch/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=3, out_channels=1, pretrained=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ukr9xgFa7oRw"
      },
      "outputs": [],
      "source": [
        "class CoinDataset(Dataset):\n",
        "  def __init__(self, image_list: list, patch_size: int, transform=None):\n",
        "    super(Dataset, self).__init__()\n",
        "    self.transform = transform\n",
        "    self.patch_size = patch_size\n",
        "    self.path_images = image_list\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.path_images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    original_path = os.path.join(self.path_images[index], \"original\", \"coin.jpeg\")\n",
        "    no_background_path = os.path.join(self.path_images[index], \"no_bg\", \"coin.jpeg\")\n",
        "\n",
        "    original_image = Image.open(original_path).convert(\"RGB\")\n",
        "    no_background_image = Image.open(no_background_path).convert(\"L\")\n",
        "    no_background_image = np.asarray(no_background_image)\n",
        "    no_background_image = (no_background_image != 0) * 1.0\n",
        "    no_background_image = Image.fromarray(no_background_image)\n",
        "\n",
        "    if self.transform:\n",
        "      original_image, no_background_image = self.transform((original_image, no_background_image))\n",
        "\n",
        "    original_image = resize(original_image, (self.patch_size, self.patch_size), antialias=True)\n",
        "    no_background_image = resize(no_background_image, (self.patch_size, self.patch_size), antialias=True)\n",
        "\n",
        "    return original_image, no_background_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentation\n",
        "In light of the dataset's lack of images, a deliberate strategy was employed to incorporate augmentation methods. This strategic decision was motivated by the need to mitigate issues related to overfitting and underfitting. \\\\\n",
        "\n",
        "To ensure uniform application of the desired alterations to input images and their corresponding ground truths, a sophisticated approach was adopted involving customising Transform modules within the PyTorch library."
      ],
      "metadata": {
        "id": "O3sPkRrBtbB6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VHbPN7R3RZK_"
      },
      "outputs": [],
      "source": [
        "class CustomRandomHorizontalFlip(RandomHorizontalFlip):\n",
        "\n",
        "  def __call__(self, samples):\n",
        "    image, gt = samples\n",
        "    if random.random() < self.p:\n",
        "      image = hflip(image)\n",
        "      gt = hflip(gt)\n",
        "    return image, gt\n",
        "\n",
        "class CustomRandomVerticalFlip(RandomVerticalFlip):\n",
        "\n",
        "  def __call__(self, samples):\n",
        "    image, gt = samples\n",
        "    if random.random() < self.p:\n",
        "      image = vflip(image)\n",
        "      gt = vflip(gt)\n",
        "    return image, gt\n",
        "\n",
        "class CustomToTensor(ToTensor):\n",
        "\n",
        "  def __call__(self, samples):\n",
        "    image, gt = samples\n",
        "    image = super().__call__(image)\n",
        "    gt = super().__call__(gt)\n",
        "    return image, gt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n6mYYLPe8xJS"
      },
      "outputs": [],
      "source": [
        "# Reproducibility\n",
        "seed_value = 12\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "# Configure\n",
        "transform = Compose([\n",
        "    CustomToTensor(),\n",
        "    CustomRandomHorizontalFlip(),\n",
        "    CustomRandomVerticalFlip(),\n",
        "])\n",
        "\n",
        "# Load paths\n",
        "root_dir = \"public_coin_dataset\"\n",
        "paths = [os.path.join(root_dir, path) for path in os.listdir(root_dir)]\n",
        "random.shuffle(paths)\n",
        "split_index = int(len(paths) * 70 / 100)\n",
        "train_paths = paths[:split_index]\n",
        "test_paths = paths[split_index:]\n",
        "\n",
        "training_batch_size = 16\n",
        "dev_batch_size = 4\n",
        "training_data = CoinDataset(image_list = train_paths, patch_size = 384, transform=transform)\n",
        "train_dataloader = DataLoader(training_data, batch_size=training_batch_size, shuffle=True)\n",
        "test_data = CoinDataset(test_paths, patch_size = 384, transform=CustomToTensor())\n",
        "test_dataloader = DataLoader(test_data, batch_size=dev_batch_size, shuffle=False)\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to('cuda')\n",
        "    torch.cuda.manual_seed(seed_value)\n",
        "    device = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "The dataset was methodically split into two segments, with a distribution ratio of 70% for training data and 30% for testing data. This division was meticulously designed to facilitate effective model training and comprehensive testing. \\\\\n",
        "\n",
        "In the context of this project, the choice of loss function fell upon MSE (Mean Squared Error) due to its suitability for the task. Additionally, RMSProp was selected as the optimizer to fine-tune the model's performance.\""
      ],
      "metadata": {
        "id": "RsIWRucrtySK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "epochs = 30\n",
        "best_accuracy = 0.0\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "# Load pre-trained if available\n",
        "if os.path.exists(\"model_checkpoints.pth\"):\n",
        "  checkpoint = torch.load(\"model_checkpoints.pth\")\n",
        "  model.load_state_dict(checkpoint[\"checkpoints\"])\n",
        "  optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "  best_accuracy = checkpoint[\"best_accuracy\"]\n",
        "  print(\"Loaded Pre-trained\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  training_loss, dev_loss, accuracy = 0., 0., 0.\n",
        "\n",
        "  model.train(True)\n",
        "  for (inputs, targets) in train_dataloader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(inputs)\n",
        "    loss = loss_fn(predictions, targets)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    training_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for (inputs, targets) in test_dataloader:\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      predictions = model(inputs)\n",
        "      dev_loss += loss_fn(predictions, targets).item() * inputs.size(0)\n",
        "      acc = (torch.round(predictions) == targets).float().mean()\n",
        "      accuracy += acc\n",
        "\n",
        "  avg_accuracy = accuracy / len(test_dataloader) * 100\n",
        "  avg_training_loss = training_loss / len(train_dataloader.sampler)\n",
        "  avg_dev_loss = dev_loss / len(test_dataloader.sampler)\n",
        "\n",
        "  if best_accuracy < avg_accuracy:\n",
        "    best_accuracy = avg_accuracy\n",
        "    torch.save({\n",
        "        'checkpoints': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_accuracy': best_accuracy\n",
        "        }, \"model_checkpoints.pth\")\n",
        "\n",
        "  print(f\"EPOCH [{epoch + 1}/{epochs}] TRAIN LOSS: {avg_training_loss:>7f}, DEV LOSS: {avg_dev_loss:>7f}, Accuracy: {avg_accuracy:.2f}%, Best Accuracy: {best_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "svqsMgt8mHBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Show Results"
      ],
      "metadata": {
        "id": "eyCYrzqzux4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show Results\n",
        "dataloader = DataLoader(\n",
        "    CoinDataset(random.choices(test_paths, k = 8), patch_size=384, transform=CustomToTensor()),\n",
        "    batch_size=1,\n",
        "    shuffle=False\n",
        ")\n",
        "accuracy = 0.\n",
        "if os.path.exists(\"model_checkpoints_pretrained.pth\"):\n",
        "  checkpoint = torch.load(\"model_checkpoints_pretrained.pth\")\n",
        "  model.load_state_dict(checkpoint[\"checkpoints\"])\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, (inputs, targets) in enumerate(dataloader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    predictions = model(inputs)\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=3)\n",
        "    ax[0].imshow(inputs[0].cpu().swapaxes(0, 2))\n",
        "    ax[0].set_title(\"Original\")\n",
        "    ax[1].imshow(predictions[0].cpu().swapaxes(0, 2))\n",
        "    ax[1].set_title(\"Predicted\")\n",
        "    ax[2].imshow(targets[0].cpu().swapaxes(0, 2))\n",
        "    ax[2].set_title(\"Ground Truth\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "G4bQ3IttLrNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsv9A8dxdiyI"
      },
      "source": [
        "# Hough Transform\n",
        "\n",
        "An alternative approach was explored during the experimentation phase, which diverged from deep learning methodologies and instead leveraged the OpenCV Hough Transform function for circle detection.\n",
        "\n",
        "Although this approach resulted in a slightly lower accuracy, it demonstrated a notable advantage in terms of execution speed, making it a pragmatic choice for specific scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yie6w16edj5p"
      },
      "outputs": [],
      "source": [
        "def apply_hough_transform(image_path: os.path) -> np.ndarray:\n",
        "  if not os.path.exists(image_path):\n",
        "    return\n",
        "\n",
        "  original_path = os.path.join(image_path, \"original\", \"coin.jpeg\")\n",
        "  original_image = cv2.imread(original_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "  # Elaborate circle\n",
        "  grayscale_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
        "  blurred_image = cv2.GaussianBlur(grayscale_image, (5, 5), 0)\n",
        "  rows = blurred_image.shape[0]\n",
        "  circles = cv2.HoughCircles(blurred_image, cv2.HOUGH_GRADIENT, 1.5, rows / 2)\n",
        "\n",
        "  # Detect bigger cicle\n",
        "  big_circle = circles[0][0]\n",
        "  center = (int(np.floor(big_circle[0])), int(np.floor(big_circle[1])))\n",
        "  radius = int(np.floor(big_circle[2]))\n",
        "  result_image = original_image.copy()\n",
        "  cv2.circle(result_image, center, radius, (255, 255, 255), thickness=cv2.FILLED)\n",
        "\n",
        "  bin_result = ((result_image == (255, 255, 255)) * (255, 255, 255))[:, :, 0]\n",
        "  return bin_result\n",
        "\n",
        "\n",
        "root_dir = \"public_coin_dataset\"\n",
        "path_images = [os.path.join(root_dir, path) for path in os.listdir(root_dir)]\n",
        "random.shuffle(path_images)\n",
        "\n",
        "accuracy = 0.0\n",
        "not_detected = 0\n",
        "\n",
        "for image_path in path_images:\n",
        "  try:\n",
        "    no_background_path = os.path.join(image_path, \"no_bg\", \"coin.jpeg\")\n",
        "    no_background = cv2.imread(no_background_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    bin_result = apply_hough_transform(image_path)\n",
        "    bin_background = (no_background != 0) * 255\n",
        "\n",
        "    accuracy += (bin_result == bin_background).mean()\n",
        "\n",
        "  except Exception:\n",
        "    not_detected += 1\n",
        "    continue\n",
        "\n",
        "avg_accuracy = accuracy / len(path_images) * 100\n",
        "print(f\"Accuracy: {avg_accuracy:.2f}%\")\n",
        "print(f\"Number of not detected image: {not_detected}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2K45E2AhzXy"
      },
      "outputs": [],
      "source": [
        "image_path = random.choice(path_images)\n",
        "\n",
        "original_path = os.path.join(image_path, \"original\", \"coin.jpeg\")\n",
        "original_image = cv2.imread(original_path, cv2.IMREAD_COLOR)\n",
        "no_background_path = os.path.join(image_path, \"no_bg\", \"coin.jpeg\")\n",
        "no_background = cv2.imread(no_background_path, cv2.IMREAD_GRAYSCALE)\n",
        "bin_result = apply_hough_transform(image_path)\n",
        "bin_background = (no_background != 0) * 255\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=4)\n",
        "ax[0].imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))\n",
        "ax[0].set_title(\"Original\")\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(bin_result, cmap = 'gray')\n",
        "ax[1].set_title(\"Result\")\n",
        "ax[1].axis('off')\n",
        "\n",
        "ax[2].imshow(bin_background, cmap = 'gray')\n",
        "ax[2].set_title(\"Ground Truth\")\n",
        "ax[2].axis('off')\n",
        "\n",
        "ax[3].imshow(bin_result - bin_background, cmap = 'gray')\n",
        "ax[3].set_title(\"Error\")\n",
        "ax[3].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YPZEZSFcHaqx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}